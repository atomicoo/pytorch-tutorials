## ================================== ##
##         AUTOGRAD MECHANICS         ##
## ================================== ##

## å‚è€ƒèµ„æ–™
#  https://huggingface.co/transformers/quicktour.html

## ç¼–ç¨‹ç¯å¢ƒ
#  OSï¼šWindows 10 Pro
#  Editorï¼šVS Code
#  Pythonï¼š3.7.7 (Anaconda 5.3.0)
#  PyTorchï¼š1.5.1

__author__ = 'Atomicoo'

# %%
import torch
import torch.nn as nn
import torch.nn.functional as F

from pathlib import Path


## @@@@@@@@@@@@@@
## Quick tour

################################
## Getting started with pipeline
###
# åœ¨ç»™å®šä»»åŠ¡ä¸Šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ€ç®€å•ç²—æš´çš„æ–¹å¼æ˜¯`pipeline()`
#   - Sentiment analysis
#   - Text generation (in English)
#   - Name entity recognition (NER)
#   - Name entity recognition (NER)
#   - Filling masked text
#   - Summarization
#   - Translation
#   - Feature extraction
# %%
from transformers import pipeline

# %%
# Pipeline
classifier = pipeline('sentiment-analysis')

# %%
results = classifier([
    "We are very happy to show you the Transformers library.", 
    "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

# %%
# æ¨¡å‹ä¸­å¿ƒï¼šhttps://huggingface.co/models
# å¯ä»¥æ ¹æ®éœ€è¦åœ¨æ¨¡å‹ä¸­å¿ƒæŸ¥è¯¢ç›¸åº”çš„æ¨¡å‹
# è­¬å¦‚ï¼š
# classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")

# %%
# ä»»åŠ¡æ±‡æ€»ï¼šhttps://huggingface.co/transformers/task_summary.html
# æŸ¥è¯¢å¸¸è§NLPä»»åŠ¡å¯¹åº”çš„æ¨¡å‹åŠå…¶ä½¿ç”¨æ–¹å¼

# %%
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel

# %%
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

# %%
# å¾®è°ƒæ¨¡å‹ç¤ºä¾‹ï¼šhttps://huggingface.co/transformers/examples.html


################################
## Under the hood: pretrained models
###
# ä½¿ç”¨`.from_pretrained()`åˆ›å»º`model`å’Œ`tokenizer`
# %%
# åˆ›å»ºmodelå’Œtokenizer
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# %%
# ä½¿ç”¨tokenizer
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(inputs)

# %%
# ä½¿ç”¨tokenizerï¼ˆæ›´å¤šå‚æ•°çš„ä½¿ç”¨ï¼‰
inputs = tokenizer(
            [ "We are very happy to show you the Transformers library.",
              "We hope you don't hate it." ],
            padding=True,       # æ˜¯å¦å¡«å……ï¼Œç›¸åŒé•¿åº¦
            truncation=True,    # æ˜¯å¦æˆªæ–­ï¼Œç›¸åŒé•¿åº¦
            return_tensors="pt")
for key, value in inputs.items():
    print(f"{key}: {value.numpy().tolist()}")

# %%
# æ¨¡å‹æ˜¯æ ‡å‡†çš„`torch.nn.Module`æ¨¡å‹ï¼ˆè‹¥ä½¿ç”¨TensorFlowåˆ™æ˜¯`tf.keras.Model`æ¨¡å‹ï¼‰
# å¯ä»¥ä½¿ç”¨PyTorchçš„æ–¹å¼æŸ¥çœ‹å®ƒçš„æ‰€æœ‰å‚æ•°
for param in model.named_parameters():
    print(param[0], param[1].size())

# %%
# ä½¿ç”¨model
outputs = model(**inputs)
print(outputs)

# %%
# è¿™é‡Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªtupleï¼Œè¿˜éœ€è¦åº”ç”¨softmaxæ¥è·å¾—æœ€ç»ˆé¢„æµ‹
predictions = F.softmax(outputs[0], dim=-1)
print(predictions)

# %%
# Transformersè¿˜æä¾›ä¸€ä¸ª`Trainer`ç±»ç”¨äºè®­ç»ƒï¼ˆå¾®è°ƒï¼‰æ¨¡å‹
# è¯¦è§ï¼šhttps://huggingface.co/transformers/training.html

# %%
# å¾®è°ƒæ¨¡å‹åï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿å­˜ï¼ˆè¿åŒtokenizerï¼‰
SAVE_DIR = Path("pretrained/distilbert-base-uncased-finetuned-sst-2-english")
tokenizer.save_pretrained(SAVE_DIR)
model.save_pretrained(SAVE_DIR)

# %%
# ç„¶åå¯ä»¥åœ¨éœ€è¦æ—¶é€šè¿‡`.from_pretrained()`é‡æ–°åŠ è½½
tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)
model = AutoModel.from_pretrained(SAVE_DIR)

# åœ¨TensorFlowä¸­åŠ è½½PyTorchæ¨¡å‹ï¼š
# model = TFAutoModel.from_pretrained(SAVE_DIR, from_pt=True)
# åœ¨PyTorchä¸­åŠ è½½TensorFlowæ¨¡å‹ï¼š
# model = AutoModel.from_pretrained(SAVE_DIR, from_tf=True)

# %%
# è¿˜å¯ä»¥ä»¤æ¨¡å‹è¿”å›æ‰€æœ‰éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡
outputs = model(**inputs, output_hidden_states=True, output_attentions=True)
hidden_states, attentions = outputs[-2:]

# %%
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# æ ¹æ®å·²çŸ¥æ¨¡å‹æ¶æ„é€‰æ‹©å¯¹åº”çš„æ¨¡å‹ç±»è¿›è¡Œå®ä¾‹åŒ–ï¼Œä¸ä¸Šè¿°ä½¿ç”¨`AutoModel`æ•ˆæœä¸€æ ·
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name)

# %%
# è‡ªå®šä¹‰æ¨¡å‹
# æ¯ç§æ¨¡å‹æ¶æ„éƒ½æœ‰ç›¸åº”çš„é…ç½®ç±»ï¼ˆè­¬å¦‚ï¼ŒCustomizing the model -> DistilBertConfigï¼‰
# å¯ä»¥é€šè¿‡ä¿®æ”¹é…ç½®å‚æ•°ï¼ˆåŒ…æ‹¬hidden dimension, dropout rateç­‰ï¼‰æ”¹å˜æ¨¡å‹ï¼Œ

from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification

# %%
# è‹¥è¿›è¡Œæ ¸å¿ƒä¿®æ”¹ï¼ˆè­¬å¦‚hidden dimensionï¼‰ï¼Œåˆ™æ— æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒ
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification(config)

# %%
# è‹¥ä»…ä¿®æ”¹æ¨¡å‹çš„å¤´éƒ¨ï¼ˆä¸‹æ¸¸ä»»åŠ¡éƒ¨åˆ†ï¼Œè­¬å¦‚ä¿®æ”¹åˆ†ç±»æ ‡ç­¾æ•°ï¼‰ï¼Œåˆ™ä»ç„¶å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

# %%
extractor = pipeline("feature-extraction")

# %%
features = extractor(
    [ "We are very happy to show you the Transformers library.",
      "We hope you don't hate it." ])
print(torch.tensor(features).size())


## @@@@@@@@@@@@@@
## Philosophy

################################
## Main concepts
###
# æ¯ç§æ¨¡å‹å›´ç»•3ä¸ªç±»æ„å»ºï¼š
#   - Model æ¨¡å‹ç±»ï¼šå„ç§æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæƒé‡æˆ–ä»å¤´è®­ç»ƒæƒé‡
#   - Config é…ç½®ç±»ï¼šModelç±»çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„çš„æƒ…å†µä¸‹æ¨¡å‹å°†è‡ªåŠ¨å®ä¾‹åŒ–é…ç½®
#   - Tokenizer ç±»ï¼šå­˜å‚¨æ¨¡å‹è¯æ±‡è¡¨ï¼Œæä¾›ç¼–/è§£ç è¾“å…¥çš„æ–¹æ³•
# åœ¨ä¸Šè¿°3ä¸ªç±»åŸºç¡€ä¸Šï¼Œè¿˜æä¾›äº†2ä¸ªAPIï¼š
#   - pipeline() ç”¨äºå¿«é€Ÿåœ°ä½¿ç”¨æ¨¡å‹
#   - Trainer() ç”¨äºæ–¹ä¾¿åœ°è®­ç»ƒ/å¾®è°ƒæ¨¡å‹
# ä¿å­˜/åŠ è½½æ¨¡å‹ï¼š
#   - from_pretrained()
#   - save_pretrained()

# %%
