{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NEURAL NETWORKS\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "> https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.5.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1128057769511200"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.random.seed()"
   ]
  },
  {
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络的典型训练过程如下：\n",
    "\n",
    "- 定义具有一些可学习参数（或权重）的神经网络\n",
    "- 遍历输入数据集\n",
    "- 通过网络处理输入\n",
    "- 计算损失（输出正确的距离有多远）\n",
    "- 将梯度传播回网络参数\n",
    "- 通常使用简单的更新规则来更新网络的权重： weight = weight - learning_rate * gradient\n",
    "\n",
    "### 定义网络\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义卷积层\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(16*6*6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1x32x32 -> 6x30x30 -> 6x15x15\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # 6x15x15 -> 16x13x13 -> 16x6x6\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # 16x6x6 -> 576\n",
    "        x = x.view(-1, self.num_flat_feats(x))\n",
    "        # 576 -> 120\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 120 -> 84\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # 84 -> 10\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_feats(self, x):\n",
    "        sz = x.size()[1:]\n",
    "        num = 1\n",
    "        for s in sz: num *= s\n",
    "        return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=576, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\nconv1.weight torch.Size([6, 1, 3, 3])\nconv1.bias torch.Size([6])\nconv2.weight torch.Size([16, 6, 3, 3])\nconv2.bias torch.Size([16])\nfc1.weight torch.Size([120, 576])\nfc1.bias torch.Size([120])\nfc2.weight torch.Size([84, 120])\nfc2.bias torch.Size([84])\nfc3.weight torch.Size([10, 84])\nfc3.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 可学习参数\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for param in net.named_parameters():\n",
    "    print(param[0], param[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.0638, -0.0186, -0.0626, -0.0354, -0.0327,  0.1017,  0.0080, -0.1287,\n          0.0306,  0.1064],\n        [-0.0621, -0.0091, -0.0458, -0.0277, -0.0204,  0.1034,  0.0251, -0.1382,\n          0.0444,  0.0991],\n        [-0.0605, -0.0160, -0.0579, -0.0329, -0.0259,  0.0981,  0.0044, -0.1230,\n          0.0498,  0.1089],\n        [-0.0605, -0.0278, -0.0669, -0.0455, -0.0112,  0.0977,  0.0273, -0.1443,\n          0.0490,  0.1125]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 随机输入\n",
    "inp = torch.randn(4, 1, 32, 32)\n",
    "outp = net(inp)\n",
    "print(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于梯度是累加的，反向传播之前需要先将梯度缓冲区清零\n",
    "net.zero_grad()\n",
    "outp.backward(torch.randn(4, 10))"
   ]
  },
  {
   "source": [
    "注意：`torch.nn`仅支持批次输入，因此`nn.Conv2D()`接收4-D张量(nSamples x nChannels x Height x Width)  \n",
    "其中第一个维度是批次尺寸。当仅有一条数据时可以调用`.unsqueeze(0)`生成“伪轴”  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "简单回顾：\n",
    "\n",
    "- `torch.Tensor`: 多维数组，支持autograd操作，并保存梯度  \n",
    "- `nn.Module`: 神经网络模块。封装参数及移动到设备、导出、加载等辅助方法  \n",
    "- `nn.Parameter`: 一种Tensor，将其作为属性分配给Module时会自动注册为参数  \n",
    "- `autograd.Function`: 实现autograd操作的前/后向定义。每个Tensor操作都至少创建一个Function节点，该节点连接到创建Tensor的函数并编码其历史  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 损失函数\n",
    "\n",
    "更多 `torch.nn` 中定义的损失函数，参见 https://pytorch.org/docs/nn.html#loss-functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = net(inp)\n",
    "tgt = torch.randn(10).view(1, -1)\n",
    "criterion = nn.MSELoss()   # 定义 MSELoss 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.7632, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(outp, tgt)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<MseLossBackward object at 0x000002801FBB0C88>\n<AddmmBackward object at 0x000002801FB157C8>\n<ReluBackward0 object at 0x000002801F564F08>\n"
     ]
    }
   ],
   "source": [
    "# 反向传播过程：\n",
    "# input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "#       -> view -> linear -> relu -> linear -> relu -> linear\n",
    "#       -> MSELoss\n",
    "#       -> loss\n",
    "print(loss.grad_fn)   # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])   # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0])   # ReLU"
   ]
  },
  {
   "source": [
    "### 反向传播\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([ 0.0079,  0.0031,  0.0038, -0.0004, -0.0031,  0.0023])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播之前需要先将梯度缓冲区清零\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "# 根据名称获取参数：`.conv1.weight`、`.conv1.bias`\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "# 根据名称获取参数：`.conv1.weight`、`.conv1.bias`\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "source": [
    "### 权重更新\n",
    "\n",
    "#### 手动更新\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for param in net.parameters():\n",
    "    # 注意！！！在`param.data`上操作而不是`param`\n",
    "    param.data.sub_(param.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "# 权重更新时，不要追踪操作，因此在`param.data`或`param.detach()`上更新\n",
    "param = next(net.parameters())\n",
    "# `param`与`param.data`的区别\n",
    "print(param.requires_grad)\n",
    "print(param.data.requires_grad)\n",
    "# 或者 `param.detach().requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias before update\nParameter containing:\ntensor([ 0.0121,  0.2160, -0.0436, -0.2049, -0.3043, -0.1758],\n       requires_grad=True)\nconv1.bias after update\nParameter containing:\ntensor([ 0.0113,  0.2157, -0.0440, -0.2049, -0.3040, -0.1760],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 权重更新前后对比\n",
    "param = net.conv1.bias\n",
    "print('conv1.bias before update')\n",
    "print(param)\n",
    "param.data.sub_(param.grad.data * 0.1)\n",
    "print('conv1.bias after update')\n",
    "print(param)"
   ]
  },
  {
   "source": [
    "#### 自动更新\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias before update\nParameter containing:\ntensor([ 0.0112,  0.2157, -0.0441, -0.2050, -0.3040, -0.1760],\n       requires_grad=True)\nconv1.bias after update\nParameter containing:\ntensor([ 0.0113,  0.2158, -0.0441, -0.2052, -0.3040, -0.1759],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# torch.optim包 - 优化器\n",
    "# 创建优化器，需要传入网络参数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "# 训练Loops中使用优化器\n",
    "optimizer.zero_grad()   # 所有梯度清零（包括网络中的参数）\n",
    "outp = net(inp)\n",
    "loss = criterion(outp, tgt)\n",
    "loss.backward()     # 反向传播\n",
    "\n",
    "param = net.conv1.bias\n",
    "print('conv1.bias before update')\n",
    "print(param)\n",
    "\n",
    "optimizer.step()    # 更新权重\n",
    "\n",
    "print('conv1.bias after update')\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}