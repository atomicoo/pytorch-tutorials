{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LEARNING PYTORCH WITH EXAMPLES\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "> https://pytorch.org/tutorials/beginner/pytorch_with_examples.html  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.5.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "147127536669100"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.random.seed()"
   ]
  },
  {
   "source": [
    "## 张量\n",
    "\n",
    "### Numpy: Array\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 876.6382\n",
      "200 10.4804\n",
      "300 0.1798\n",
      "400 0.0034\n",
      "500 0.0001\n"
     ]
    }
   ],
   "source": [
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # forward\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    # print loss\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss))\n",
    "    # backward\n",
    "    grad_y_pred = 2.*(y_pred-y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    # update weight\n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2"
   ]
  },
  {
   "source": [
    "### PyTorch: Tensor\n",
    "\n",
    "> `ndarray` -> `tensor`  \n",
    ">> `ndarray.dot()` -> `tensor.mm()`  \n",
    ">> `ndarray.maximum()` -> `tensor.clamp()`  \n",
    ">> `ndarray.T` -> `tensor.t()`  \n",
    ">> `ndarray.square()` -> `tensor.pow(2)`  \n",
    ">> `ndarray.copy()` -> `tensor.clone()`  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 585.0891\n200 4.3040\n300 0.0422\n400 0.0007\n500 0.0001\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # forward\n",
    "    h = x.mm(w1)\n",
    "    # torch.clamp(input, min, max, out=None) -> Tensor\n",
    "    # 将input张量每个元素夹紧到[min,max]区间内\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    # print loss\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))\n",
    "    # backward\n",
    "    grad_y_pred = 2.*(y_pred-y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # update weight\n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2"
   ]
  },
  {
   "source": [
    "## 自动微分\n",
    "\n",
    "### 张量与自动微分\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 622.4977\n",
      "200 4.3769\n",
      "300 0.0473\n",
      "400 0.0009\n",
      "500 0.0001\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # forward\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    # print loss\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))   # `.item()`获取单个元素Tensor的标量值\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # update weight\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate*w1.grad\n",
    "        w2 -= learning_rate*w2.grad\n",
    "        # manually zero gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "source": [
    "### 自定义Autograd函数\n",
    "\n",
    "可通过继承 `torch.autograd.Function` 来实现自定义Autograd函数。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"自定义ReLU\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        ouputs = inputs.clamp(min=0)\n",
    "        ctx.save_for_backward(inputs)\n",
    "        return ouputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        inputs, = ctx.saved_tensors\n",
    "        grad_inputs = grad_outputs.clone()\n",
    "        grad_inputs[inputs<0] = 0\n",
    "        return grad_inputs\n",
    "\n",
    "\n",
    "class MyMM(torch.autograd.Function):\n",
    "    \"\"\"自定义矩阵乘法\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        # N*I I*O  N*O\n",
    "        ouputs = inputs.mm(weights)\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return ouputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        grad_inputs = grad_outputs.mm(weights.t())\n",
    "        grad_weights = inputs.t().mm(grad_outputs)\n",
    "        return grad_inputs, grad_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 433.6183\n",
      "200 1.8319\n",
      "300 0.0107\n",
      "400 0.0002\n",
      "500 0.0000\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    # 自定义Autograd函数的使用，调用`Function.apply()`方法\n",
    "    mm = MyMM.apply\n",
    "    relu = MyReLU.apply\n",
    "    # forward\n",
    "    y_pred = mm(relu(mm(x, w1)), w2)\n",
    "    # print loss\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))   # `.item()`获取单个元素Tensor的标量值\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # update weight\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate*w1.grad\n",
    "        w2 -= learning_rate*w2.grad\n",
    "        # manually zero gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "source": [
    "## nn.Module\n",
    "\n",
    "`nn.Module`包定义了一组Modules，大致等效于神经网络的Layers，对神经网络进行更高级别的抽象。  \n",
    "此外，`nn.Module`包还定义了一组常用的损失函数。  \n",
    "\n",
    "### PyTorch: nn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "100 1.9401\n",
      "200 0.0394\n",
      "300 0.0021\n",
      "400 0.0002\n",
      "500 0.0000\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 将神经网络定义为Layers的序列（Sequence）\n",
    "model = torch.nn.Sequential(OrderedDict([\n",
    "    ('linear1', torch.nn.Linear(D_in, H)),\n",
    "    ('relu', torch.nn.ReLU()),\n",
    "    ('linear2', torch.nn.Linear(H, D_out)),\n",
    "]))\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数为均方差（Mean Squared Error, MSE）\n",
    "# reduction = 'none'|'mean'|'sum'\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-4\n",
    "for t in range(1, 501):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))\n",
    "    # zero gradients\n",
    "    model.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate*param.grad"
   ]
  },
  {
   "source": [
    "### PyTorch: optim\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "100 49.7564\n",
      "200 0.8396\n",
      "300 0.0053\n",
      "400 0.0000\n",
      "500 0.0000\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 将神经网络定义为Layers的序列（Sequence）\n",
    "model = torch.nn.Sequential(OrderedDict([\n",
    "    ('linear1', torch.nn.Linear(D_in, H)),\n",
    "    ('relu', torch.nn.ReLU()),\n",
    "    ('linear2', torch.nn.Linear(H, D_out)),\n",
    "]))\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数为均方差（Mean Squared Error, MSE）\n",
    "# reduction = 'none'|'mean'|'sum'\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(1, 501):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### 自定义nn.Module\n",
    "\n",
    "可通过继承 `torch.nn.Module` 来实现自定义Modules。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单双层网络模型 SimpleNet\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    \"\"\"自定义SimpleNet\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 初始化模型，定义好Layers\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 重写`.forward()`方法，接收模型输入\n",
    "        hidden = self.linear1(inputs)\n",
    "        hidden_relu = self.relu(hidden)\n",
    "        y_pred = self.linear2(hidden_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleNet(\n",
      "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "100 47.0855\n",
      "200 0.7062\n",
      "300 0.0043\n",
      "400 0.0000\n",
      "500 0.0000\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 将神经网络定义为Layers的序列（Sequence）\n",
    "model = SimpleNet()\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数为均方差（Mean Squared Error, MSE）\n",
    "# reduction = 'none'|'mean'|'sum'\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(1, 501):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "source": [
    "### 控制Flow和权重共享\n",
    "\n",
    "动态网络模型：由于PyTorch使用的是动态图，因此可以轻易地实现动态网络模型，即模型的网络结构在迭代里可以动态地变化。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    \"\"\"自定义动态网络\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_linear = torch.nn.Linear(D_in, H)\n",
    "        self.mid_linear = torch.nn.Linear(H, H)\n",
    "        self.out_linear = torch.nn.Linear(H, D_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_relu = self.relu(self.in_linear(inputs))\n",
    "        # 随机确定0~2个隐层\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            hidden_relu = self.relu(self.mid_linear(hidden_relu))\n",
    "        y_pred = self.out_linear(hidden_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DynamicNet(\n",
      "  (in_linear): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (mid_linear): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (out_linear): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "100 592.9760\n",
      "200 300.9666\n",
      "300 122.1668\n",
      "400 16.4641\n",
      "500 9.5828\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N: batch_size; D_in: dim of input\n",
    "# H: dim of hidden; D_out: dim of output\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机输入输出\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 将神经网络定义为Layers的序列（Sequence）\n",
    "model = DynamicNet()\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数为均方差（Mean Squared Error, MSE）\n",
    "# reduction = 'none'|'mean'|'sum'\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 开始训练模型\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(1, 501):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 0:\n",
    "        print('%3d %.4f' % (t, loss.item()))\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}