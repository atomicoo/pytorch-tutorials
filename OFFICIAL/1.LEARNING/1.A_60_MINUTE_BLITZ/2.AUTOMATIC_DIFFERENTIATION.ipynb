{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "> https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py  \n",
    "> https://atomicoo.com/technology/pytorch-auto-diff-autograd/  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.5.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1124283013989400"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.random.seed()"
   ]
  },
  {
   "source": [
    "## 自动求导\n",
    "\n",
    "AutoGrad包是PyTorch中所有神经网络的核心，为张量上的所有操作提供自动求导。  \n",
    "它是一个运行时定义的框架，即反向传播是随着对张量的操作来逐步决定的，这也意味着在每个迭代中都可以是不同的。  \n",
    "\n",
    "### 张量\n",
    "\n",
    "`torch.Tensor` 类的重要属性/方法：\n",
    "  - dtype:   该张量存储的值类型，可选类型见：`torch.dtype`；\n",
    "  - device:  该张量存放的设备类型，cpu/gpu\n",
    "  - data:    该张量节点存储的值；\n",
    "  - requires_grad: 表示autograd时是否需要计算此tensor的梯度，默认False；\n",
    "  - grad:    存储梯度的值，初始为None；\n",
    "  - grad_fn: 反向传播时，用来计算梯度的函数；\n",
    "  - is_leaf: 该张量节点在计算图中是否为叶子节点；"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 将`.requires_grad`设置为true，追踪Tensor上的所有操作\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x00000260CE227108>\n"
     ]
    }
   ],
   "source": [
    "# `.grad_fn`属性指向Function，编码Tensor间的运算，\n",
    "# 包含`.forward()`和`.backward()`\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\ntensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nTrue\n<SumBackward0 object at 0x00000260CF1EFEC8>\n"
     ]
    }
   ],
   "source": [
    "# 就地改变`.requires_grad`属性\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[9., 9.],\n        [9., 9.]], requires_grad=True)\ntensor([[11., 11.],\n        [11., 11.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 通过调用`.detach()`阻止追踪Tensor操作（历史记录）\n",
    "# 或通过将代码块包裹在`with torch.no_grad():`中（评估模型时常用）\n",
    "# 否则所有Tensor操作（包括拷贝、填充等）都会被追踪\n",
    "a.detach().fill_(9.)\n",
    "print(a)\n",
    "# or\n",
    "with torch.no_grad():\n",
    "    a[:] = 11.\n",
    "print(a)"
   ]
  },
  {
   "source": [
    "### 梯度\n",
    "\n",
    "关于`.backward()`为什么需要参数`grad_tensor`的存在，见下。具体参考 https://zhuanlan.zhihu.com/p/83172023  \n",
    "扩展：CLASS `torch.autograd.Function` https://pytorch.org/docs/stable/autograd.html#function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]], grad_fn=<CloneBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 调用`torch.backward()`自动反向传播计算梯度，并将梯度累积到`.grad`属性中\n",
    "# 注意，只有`.requires_grad`与`.is_leaf`同时为True的Tensor才会累积梯度\n",
    "out.backward(create_graph=True)\n",
    "# 此处`out.backward()`等价于`out.backward(torch.tens or(1.))`\n",
    "# 因为out为1x1张量，所以`.backward()`参数可以省略\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动计算梯度还可以使用以下方法调用\n",
    "torch.autograd.backward(out, create_graph=True)\n",
    "print(x.grad)\n",
    "# or\n",
    "torch.autograd.backward(out, create_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,requires_grad=True)\n",
    "z = x + 2\n",
    "z.backward(torch.ones_like(z))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 由于反向传播中梯度是进行累积的，所以当输出out为矩阵时\n",
    "# 直接反向计算梯度与先将out元素求和再反向计算梯度的结果是一样的\n",
    "# 通过设置`grad_tensor`参数可以“加权”求和\n",
    "x = torch.ones(2,requires_grad=True)\n",
    "z = x + 2\n",
    "z.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "数学上，若给定 $\\vec{y}=f(\\vec{x})$，则 $\\vec{y}$ 对 $\\vec{x}$ 的梯度是一个 Jacobian 矩阵：\n",
    "\n",
    "$$\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "若标量函数 $l=g(\\vec{y})$ 的梯度为 $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$，则 $l$ 对 $\\vec{x}$ 的梯度是如下 vector-Jacobian product：\n",
    "\n",
    "$$\n",
    "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "`torch.autograd` 就是一个计算 vector-Jacobian product 的引擎。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1680.4718, -246.4453,  961.9392], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}